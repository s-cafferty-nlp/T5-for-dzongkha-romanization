{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOA6/p18BWu9GAwGD0Z2GYF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"gIh0f5AEWuke","executionInfo":{"status":"ok","timestamp":1676898971925,"user_tz":180,"elapsed":6630,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import pandas as pd"]},{"cell_type":"code","source":["df = pd.read_csv('/content/thai_romanization.csv',delimiter='\t', nrows = 100000)\n","\n","df.to_csv('thai.txt', header=None, index=None, sep=' ', mode='a')"],"metadata":{"id":"uAv6mrnnaDQa","executionInfo":{"status":"ok","timestamp":1676899002695,"user_tz":180,"elapsed":328,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["with open('/content/thai.txt', 'r') as f:\n","    text = f.read()"],"metadata":{"id":"7VxK-MGHYWqH","executionInfo":{"status":"ok","timestamp":1676899005020,"user_tz":180,"elapsed":1,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["text[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"q7dnO_9WYmyZ","executionInfo":{"status":"ok","timestamp":1676899005487,"user_tz":180,"elapsed":6,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"952a9cdb-5d71-4fe9-a434-6a2e5332a315"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'กองพันทหารปืนใหญ่ kongphanthahanpuenyai\\nวิฑูรย์ withun\\nเมตาบอลิสม metabolisom\\nบ้านหนองเลา bannonglao'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","# encode the text\n","encoded = np.array([char2int[ch] for ch in text])"],"metadata":{"id":"EH7ueibmYn5P","executionInfo":{"status":"ok","timestamp":1676899007870,"user_tz":180,"elapsed":666,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["encoded[:100]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uSjGDcwcYp40","executionInfo":{"status":"ok","timestamp":1676899007870,"user_tz":180,"elapsed":4,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"762eee3b-ab80-42b6-dad3-904d0c1e2d5f"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 84,  38,  87,  13, 101,  75,  83,  73,   5,  64,  46,  20,  75,\n","         8,  73,   0,  33,  16,  21,  31,  34,  88,  78,  41,  69,  34,\n","        93,  41,  69,  41,  69,  34,  78,  55,  29,  34,   9,  69,  25,\n","        12,  28, 102,  76,  35,  64,  32,  81,  16,  98,  25,  93,  41,\n","        55,  34,  12,  62,  60,  53,   5,  96,  38,  89, 102, 100,  60,\n","        16,  36,  29,  93,  69,  82,  31,  40,  25,  45,  31,  36,  12,\n","        96,  54,   5,  75,  73,  75,  38,  87,  62,  89,   5,  16,  82,\n","        69,  34,  34,  31,  34,  88,  40,  69,  31])"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["def one_hot_encode(arr, n_labels):\n","    \n","    # Initialize the the encoded array\n","    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n","    \n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n","    \n","    # Finally reshape it to get back to the original array\n","    one_hot = one_hot.reshape((*arr.shape, n_labels))\n","    \n","    return one_hot"],"metadata":{"id":"I-aut1FcYrLF","executionInfo":{"status":"ok","timestamp":1676899009467,"user_tz":180,"elapsed":3,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["test_seq = np.array([[3, 5, 1]])\n","one_hot = one_hot_encode(test_seq, 8)\n","\n","print(one_hot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xc1le_0jYv5V","executionInfo":{"status":"ok","timestamp":1676899011391,"user_tz":180,"elapsed":3,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"a739abde-0720-4990-e001-2359617558fd"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"]}]},{"cell_type":"code","source":["def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","       \n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","    \n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","    \n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","    \n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y\n"],"metadata":{"id":"qKGn6oFWYyR7","executionInfo":{"status":"ok","timestamp":1676899013998,"user_tz":180,"elapsed":2,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["batches = get_batches(encoded, 8, 50)\n","x, y = next(batches)"],"metadata":{"id":"1nKclfA6Y3Oc","executionInfo":{"status":"ok","timestamp":1676899017177,"user_tz":180,"elapsed":1,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["print('x\\n', x[:10, :10])\n","print('\\ny\\n', y[:10, :10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VI5oNIMbY5XQ","executionInfo":{"status":"ok","timestamp":1676899019206,"user_tz":180,"elapsed":3,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"383bfa54-7cc5-4611-83af-04167a346102"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["x\n"," [[ 84  38  87  13 101  75  83  73   5  64]\n"," [ 75  73  75  38  87  75  85   8 100  16]\n"," [ 48  59  38  16  56  75  75   5  84  92]\n"," [  3  29  93  78  41  37  69  72  41  55]\n"," [ 93  45  69  12  96  54   5  75  50  60]\n"," [ 44  65  84  84  23  54  16  21  41  55]\n"," [ 93  21  41  69  12  96  54   5  75  56]\n"," [ 12  61  38  32  60 102  53  64  59  60]]\n","\n","y\n"," [[ 38  87  13 101  75  83  73   5  64  46]\n"," [ 73  75  38  87  75  85   8 100  16  82]\n"," [ 59  38  16  56  75  75   5  84  92  48]\n"," [ 29  93  78  41  37  69  72  41  55  34]\n"," [ 45  69  12  96  54   5  75  50  60  16]\n"," [ 65  84  84  23  54  16  21  41  55  21]\n"," [ 21  41  69  12  96  54   5  75  56  75]\n"," [ 61  38  32  60 102  53  64  59  60  53]]\n"]}]},{"cell_type":"code","source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","if(train_on_gpu):\n","    print('Training on GPU!')\n","else: \n","    print('No GPU available, training on CPU; consider making n_epochs very small.')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9ciod7QY9Rx","executionInfo":{"status":"ok","timestamp":1676899027026,"user_tz":180,"elapsed":1870,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"13dfe287-ff39-4bb2-a2ce-42baf6aef32d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU!\n"]}]},{"cell_type":"code","source":["class CharRNN(nn.Module):\n","    \n","    def __init__(self, tokens, n_hidden=256, n_layers=2,\n","                               drop_prob=0.5, lr=0.001):\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","        \n","        # creating character dictionaries\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","        \n","        ## TODO: define the LSTM\n","        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        ## TODO: define a dropout layer\n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        ## TODO: define the final, fully-connected output layer\n","        self.fc = nn.Linear(n_hidden, len(self.chars))\n","      \n","    \n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network. \n","            These inputs are x, and the hidden/cell state `hidden`. '''\n","                \n","        ## TODO: Get the outputs and the new hidden state from the lstm\n","        # print(hidden)\n","        # h0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).cuda() # 2 for bidirection \n","        # c0 = torch.zeros(self.n_layers, x.size(0), self.n_hidden).cuda()\n","        # print(h0.shape)\n","        # Forward propagate LSTM\n","        # out, _ = self.lstm(x, (h0, c0))\n","        r_output, hidden = self.lstm(x, hidden)\n","        # r_output, hidden = self.lstm(x, torch.cat((h0, c0)))\n","        \n","        ## TODO: pass through a dropout layer\n","        out = self.dropout(r_output)\n","        \n","        # Stack up LSTM outputs using view\n","        # you may need to use contiguous to reshape the output\n","        out = out.contiguous().view(-1, self.n_hidden)\n","        \n","        ## TODO: put x through the fully-connected layer\n","        out = self.fc(out)\n","        \n","        # return the final output and the hidden state\n","        return out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden\n","        \n"],"metadata":{"id":"_xEcY2hFZCBo","executionInfo":{"status":"ok","timestamp":1676902063004,"user_tz":180,"elapsed":213,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":90,"outputs":[]},{"cell_type":"code","source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n","    ''' Training a network \n","    \n","        Arguments\n","        ---------\n","        \n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","    \n","    '''\n","    net.train()\n","    \n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","    \n","    if(train_on_gpu):\n","        net.cuda()\n","    \n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","        \n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","            \n","            # One-hot encode our data and make them Torch tensors\n","            x = one_hot_encode(x, n_chars)\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            \n","            if(train_on_gpu):\n","                inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","\n","            # zero accumulated gradients\n","            net.zero_grad()\n","            \n","            # get the output from the model\n","            output, h = net(inputs, h)\n","            \n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","            \n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x = one_hot_encode(x, n_chars)\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","                    \n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","                    \n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.cuda(), targets.cuda()\n","\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n","                \n","                    val_losses.append(val_loss.item())\n","                \n","                net.train() # reset to train mode after iterationg through validation data\n","                \n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"],"metadata":{"id":"xlQUd6q7ZOuk","executionInfo":{"status":"ok","timestamp":1676902065318,"user_tz":180,"elapsed":878,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":91,"outputs":[]},{"cell_type":"code","source":["# define and print the net\n","n_hidden=512\n","n_layers=2\n","\n","net = CharRNN(chars, n_hidden, n_layers)\n","print(net)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vakNWXZzZUn6","executionInfo":{"status":"ok","timestamp":1676902065658,"user_tz":180,"elapsed":2,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"1b52b28a-b70d-4381-8e4f-94f96936c014"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["CharRNN(\n","  (lstm): LSTM(104, 512, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (fc): Linear(in_features=512, out_features=104, bias=True)\n",")\n"]}]},{"cell_type":"code","source":["batch_size = 128\n","seq_length = 100\n","n_epochs = 10 # start smaller if you are just testing initial behavior\n","\n","# train the model\n","train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfwAMszaZWoR","executionInfo":{"status":"ok","timestamp":1676902294859,"user_tz":180,"elapsed":228974,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"f086d3e4-23f3-4f0c-89af-bb540b90074d"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/10... Step: 10... Loss: 3.9956... Val Loss: 3.9424\n","Epoch: 1/10... Step: 20... Loss: 3.9036... Val Loss: 3.8873\n","Epoch: 1/10... Step: 30... Loss: 3.9059... Val Loss: 3.8805\n","Epoch: 1/10... Step: 40... Loss: 3.8914... Val Loss: 3.8784\n","Epoch: 1/10... Step: 50... Loss: 3.8979... Val Loss: 3.8685\n","Epoch: 1/10... Step: 60... Loss: 3.8365... Val Loss: 3.8058\n","Epoch: 1/10... Step: 70... Loss: 3.6324... Val Loss: 3.6892\n","Epoch: 1/10... Step: 80... Loss: 3.4367... Val Loss: 3.3892\n","Epoch: 1/10... Step: 90... Loss: 3.2803... Val Loss: 3.2130\n","Epoch: 1/10... Step: 100... Loss: 3.1405... Val Loss: 3.1090\n","Epoch: 1/10... Step: 110... Loss: 3.0654... Val Loss: 3.0518\n","Epoch: 1/10... Step: 120... Loss: 3.0011... Val Loss: 2.9714\n","Epoch: 1/10... Step: 130... Loss: 2.9374... Val Loss: 2.8946\n","Epoch: 1/10... Step: 140... Loss: 2.8555... Val Loss: 2.8001\n","Epoch: 1/10... Step: 150... Loss: 2.7765... Val Loss: 2.7259\n","Epoch: 2/10... Step: 160... Loss: 2.7278... Val Loss: 2.6714\n","Epoch: 2/10... Step: 170... Loss: 2.7017... Val Loss: 2.6203\n","Epoch: 2/10... Step: 180... Loss: 2.6359... Val Loss: 2.5836\n","Epoch: 2/10... Step: 190... Loss: 2.5785... Val Loss: 2.5451\n","Epoch: 2/10... Step: 200... Loss: 2.6114... Val Loss: 2.5807\n","Epoch: 2/10... Step: 210... Loss: 2.5523... Val Loss: 2.4944\n","Epoch: 2/10... Step: 220... Loss: 2.4921... Val Loss: 2.4645\n","Epoch: 2/10... Step: 230... Loss: 2.5092... Val Loss: 2.4299\n","Epoch: 2/10... Step: 240... Loss: 2.4781... Val Loss: 2.4171\n","Epoch: 2/10... Step: 250... Loss: 2.4200... Val Loss: 2.3861\n","Epoch: 2/10... Step: 260... Loss: 2.4274... Val Loss: 2.3595\n","Epoch: 2/10... Step: 270... Loss: 2.4051... Val Loss: 2.3465\n","Epoch: 2/10... Step: 280... Loss: 2.3777... Val Loss: 2.3280\n","Epoch: 2/10... Step: 290... Loss: 2.3705... Val Loss: 2.3744\n","Epoch: 2/10... Step: 300... Loss: 2.3838... Val Loss: 2.2891\n","Epoch: 2/10... Step: 310... Loss: 2.3221... Val Loss: 2.2686\n","Epoch: 3/10... Step: 320... Loss: 2.3188... Val Loss: 2.2593\n","Epoch: 3/10... Step: 330... Loss: 2.3318... Val Loss: 2.2499\n","Epoch: 3/10... Step: 340... Loss: 2.2918... Val Loss: 2.2271\n","Epoch: 3/10... Step: 350... Loss: 2.2503... Val Loss: 2.2204\n","Epoch: 3/10... Step: 360... Loss: 2.2747... Val Loss: 2.1905\n","Epoch: 3/10... Step: 370... Loss: 2.2293... Val Loss: 2.1643\n","Epoch: 3/10... Step: 380... Loss: 2.2181... Val Loss: 2.1451\n","Epoch: 3/10... Step: 390... Loss: 2.1876... Val Loss: 2.1371\n","Epoch: 3/10... Step: 400... Loss: 2.1564... Val Loss: 2.1141\n","Epoch: 3/10... Step: 410... Loss: 2.1618... Val Loss: 2.0875\n","Epoch: 3/10... Step: 420... Loss: 2.1138... Val Loss: 2.0645\n","Epoch: 3/10... Step: 430... Loss: 2.1094... Val Loss: 2.0512\n","Epoch: 3/10... Step: 440... Loss: 2.0765... Val Loss: 2.0473\n","Epoch: 3/10... Step: 450... Loss: 2.0978... Val Loss: 2.0371\n","Epoch: 3/10... Step: 460... Loss: 2.1033... Val Loss: 2.0023\n","Epoch: 3/10... Step: 470... Loss: 2.0055... Val Loss: 1.9715\n","Epoch: 4/10... Step: 480... Loss: 2.0298... Val Loss: 1.9795\n","Epoch: 4/10... Step: 490... Loss: 2.0600... Val Loss: 1.9525\n","Epoch: 4/10... Step: 500... Loss: 2.0158... Val Loss: 1.9261\n","Epoch: 4/10... Step: 510... Loss: 1.9444... Val Loss: 1.9040\n","Epoch: 4/10... Step: 520... Loss: 1.9452... Val Loss: 1.8760\n","Epoch: 4/10... Step: 530... Loss: 1.9479... Val Loss: 1.8620\n","Epoch: 4/10... Step: 540... Loss: 1.9086... Val Loss: 1.8396\n","Epoch: 4/10... Step: 550... Loss: 1.8603... Val Loss: 1.8291\n","Epoch: 4/10... Step: 560... Loss: 1.8718... Val Loss: 1.8048\n","Epoch: 4/10... Step: 570... Loss: 1.8787... Val Loss: 1.7675\n","Epoch: 4/10... Step: 580... Loss: 1.8189... Val Loss: 1.7560\n","Epoch: 4/10... Step: 590... Loss: 1.7996... Val Loss: 1.7463\n","Epoch: 4/10... Step: 600... Loss: 1.8026... Val Loss: 1.7237\n","Epoch: 4/10... Step: 610... Loss: 1.7862... Val Loss: 1.6965\n","Epoch: 4/10... Step: 620... Loss: 1.7595... Val Loss: 1.7012\n","Epoch: 5/10... Step: 630... Loss: 1.7830... Val Loss: 1.6689\n","Epoch: 5/10... Step: 640... Loss: 1.7222... Val Loss: 1.6505\n","Epoch: 5/10... Step: 650... Loss: 1.7112... Val Loss: 1.6360\n","Epoch: 5/10... Step: 660... Loss: 1.6850... Val Loss: 1.6303\n","Epoch: 5/10... Step: 670... Loss: 1.7006... Val Loss: 1.6101\n","Epoch: 5/10... Step: 680... Loss: 1.7030... Val Loss: 1.6200\n","Epoch: 5/10... Step: 690... Loss: 1.6496... Val Loss: 1.5933\n","Epoch: 5/10... Step: 700... Loss: 1.6597... Val Loss: 1.5711\n","Epoch: 5/10... Step: 710... Loss: 1.6895... Val Loss: 1.5542\n","Epoch: 5/10... Step: 720... Loss: 1.5844... Val Loss: 1.5451\n","Epoch: 5/10... Step: 730... Loss: 1.6660... Val Loss: 1.5347\n","Epoch: 5/10... Step: 740... Loss: 1.6246... Val Loss: 1.5339\n","Epoch: 5/10... Step: 750... Loss: 1.5940... Val Loss: 1.5141\n","Epoch: 5/10... Step: 760... Loss: 1.5610... Val Loss: 1.5045\n","Epoch: 5/10... Step: 770... Loss: 1.5626... Val Loss: 1.4953\n","Epoch: 5/10... Step: 780... Loss: 1.6021... Val Loss: 1.4890\n","Epoch: 6/10... Step: 790... Loss: 1.5413... Val Loss: 1.4695\n","Epoch: 6/10... Step: 800... Loss: 1.6001... Val Loss: 1.4698\n","Epoch: 6/10... Step: 810... Loss: 1.5357... Val Loss: 1.4542\n","Epoch: 6/10... Step: 820... Loss: 1.5328... Val Loss: 1.4473\n","Epoch: 6/10... Step: 830... Loss: 1.5218... Val Loss: 1.4466\n","Epoch: 6/10... Step: 840... Loss: 1.5304... Val Loss: 1.4337\n","Epoch: 6/10... Step: 850... Loss: 1.5060... Val Loss: 1.4276\n","Epoch: 6/10... Step: 860... Loss: 1.5202... Val Loss: 1.4290\n","Epoch: 6/10... Step: 870... Loss: 1.4664... Val Loss: 1.4146\n","Epoch: 6/10... Step: 880... Loss: 1.4993... Val Loss: 1.4105\n","Epoch: 6/10... Step: 890... Loss: 1.4737... Val Loss: 1.3971\n","Epoch: 6/10... Step: 900... Loss: 1.4849... Val Loss: 1.3893\n","Epoch: 6/10... Step: 910... Loss: 1.4655... Val Loss: 1.3877\n","Epoch: 6/10... Step: 920... Loss: 1.4254... Val Loss: 1.3803\n","Epoch: 6/10... Step: 930... Loss: 1.4555... Val Loss: 1.3762\n","Epoch: 6/10... Step: 940... Loss: 1.4230... Val Loss: 1.3791\n","Epoch: 7/10... Step: 950... Loss: 1.4356... Val Loss: 1.3654\n","Epoch: 7/10... Step: 960... Loss: 1.4350... Val Loss: 1.3612\n","Epoch: 7/10... Step: 970... Loss: 1.3952... Val Loss: 1.3521\n","Epoch: 7/10... Step: 980... Loss: 1.4124... Val Loss: 1.3473\n","Epoch: 7/10... Step: 990... Loss: 1.4135... Val Loss: 1.3504\n","Epoch: 7/10... Step: 1000... Loss: 1.4597... Val Loss: 1.3409\n","Epoch: 7/10... Step: 1010... Loss: 1.3968... Val Loss: 1.3346\n","Epoch: 7/10... Step: 1020... Loss: 1.3866... Val Loss: 1.3363\n","Epoch: 7/10... Step: 1030... Loss: 1.4150... Val Loss: 1.3335\n","Epoch: 7/10... Step: 1040... Loss: 1.3960... Val Loss: 1.3259\n","Epoch: 7/10... Step: 1050... Loss: 1.4166... Val Loss: 1.3196\n","Epoch: 7/10... Step: 1060... Loss: 1.3943... Val Loss: 1.3241\n","Epoch: 7/10... Step: 1070... Loss: 1.3625... Val Loss: 1.3192\n","Epoch: 7/10... Step: 1080... Loss: 1.3982... Val Loss: 1.3234\n","Epoch: 7/10... Step: 1090... Loss: 1.3854... Val Loss: 1.3150\n","Epoch: 8/10... Step: 1100... Loss: 1.6512... Val Loss: 1.3069\n","Epoch: 8/10... Step: 1110... Loss: 1.3749... Val Loss: 1.3000\n","Epoch: 8/10... Step: 1120... Loss: 1.3660... Val Loss: 1.2978\n","Epoch: 8/10... Step: 1130... Loss: 1.3476... Val Loss: 1.2952\n","Epoch: 8/10... Step: 1140... Loss: 1.3713... Val Loss: 1.2919\n","Epoch: 8/10... Step: 1150... Loss: 1.3437... Val Loss: 1.2980\n","Epoch: 8/10... Step: 1160... Loss: 1.3586... Val Loss: 1.2939\n","Epoch: 8/10... Step: 1170... Loss: 1.3587... Val Loss: 1.2891\n","Epoch: 8/10... Step: 1180... Loss: 1.3397... Val Loss: 1.2862\n","Epoch: 8/10... Step: 1190... Loss: 1.3158... Val Loss: 1.2829\n","Epoch: 8/10... Step: 1200... Loss: 1.3477... Val Loss: 1.2773\n","Epoch: 8/10... Step: 1210... Loss: 1.3329... Val Loss: 1.2709\n","Epoch: 8/10... Step: 1220... Loss: 1.3029... Val Loss: 1.2688\n","Epoch: 8/10... Step: 1230... Loss: 1.3511... Val Loss: 1.2690\n","Epoch: 8/10... Step: 1240... Loss: 1.3262... Val Loss: 1.2698\n","Epoch: 8/10... Step: 1250... Loss: 1.3585... Val Loss: 1.2682\n","Epoch: 9/10... Step: 1260... Loss: 1.3161... Val Loss: 1.2612\n","Epoch: 9/10... Step: 1270... Loss: 1.2934... Val Loss: 1.2554\n","Epoch: 9/10... Step: 1280... Loss: 1.3075... Val Loss: 1.2621\n","Epoch: 9/10... Step: 1290... Loss: 1.3071... Val Loss: 1.2528\n","Epoch: 9/10... Step: 1300... Loss: 1.3281... Val Loss: 1.2484\n","Epoch: 9/10... Step: 1310... Loss: 1.3484... Val Loss: 1.2495\n","Epoch: 9/10... Step: 1320... Loss: 1.3388... Val Loss: 1.2493\n","Epoch: 9/10... Step: 1330... Loss: 1.2836... Val Loss: 1.2411\n","Epoch: 9/10... Step: 1340... Loss: 1.2903... Val Loss: 1.2388\n","Epoch: 9/10... Step: 1350... Loss: 1.2856... Val Loss: 1.2411\n","Epoch: 9/10... Step: 1360... Loss: 1.2825... Val Loss: 1.2386\n","Epoch: 9/10... Step: 1370... Loss: 1.2616... Val Loss: 1.2346\n","Epoch: 9/10... Step: 1380... Loss: 1.2986... Val Loss: 1.2328\n","Epoch: 9/10... Step: 1390... Loss: 1.2994... Val Loss: 1.2363\n","Epoch: 9/10... Step: 1400... Loss: 1.2824... Val Loss: 1.2380\n","Epoch: 9/10... Step: 1410... Loss: 1.2977... Val Loss: 1.2310\n","Epoch: 10/10... Step: 1420... Loss: 1.2812... Val Loss: 1.2334\n","Epoch: 10/10... Step: 1430... Loss: 1.3003... Val Loss: 1.2243\n","Epoch: 10/10... Step: 1440... Loss: 1.2846... Val Loss: 1.2207\n","Epoch: 10/10... Step: 1450... Loss: 1.2698... Val Loss: 1.2196\n","Epoch: 10/10... Step: 1460... Loss: 1.2585... Val Loss: 1.2236\n","Epoch: 10/10... Step: 1470... Loss: 1.2905... Val Loss: 1.2279\n","Epoch: 10/10... Step: 1480... Loss: 1.2908... Val Loss: 1.2234\n","Epoch: 10/10... Step: 1490... Loss: 1.2702... Val Loss: 1.2186\n","Epoch: 10/10... Step: 1500... Loss: 1.2762... Val Loss: 1.2143\n","Epoch: 10/10... Step: 1510... Loss: 1.2493... Val Loss: 1.2132\n","Epoch: 10/10... Step: 1520... Loss: 1.2565... Val Loss: 1.2086\n","Epoch: 10/10... Step: 1530... Loss: 1.2408... Val Loss: 1.2087\n","Epoch: 10/10... Step: 1540... Loss: 1.2654... Val Loss: 1.2085\n","Epoch: 10/10... Step: 1550... Loss: 1.2568... Val Loss: 1.2033\n","Epoch: 10/10... Step: 1560... Loss: 1.2817... Val Loss: 1.2042\n","Epoch: 10/10... Step: 1570... Loss: 1.3519... Val Loss: 1.2011\n"]}]},{"cell_type":"code","source":["model_name = 'rnn_20_epoch.net'\n","\n","checkpoint = {'n_hidden': net.n_hidden,\n","              'n_layers': net.n_layers,\n","              'state_dict': net.state_dict(),\n","              'tokens': net.chars}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)"],"metadata":{"id":"Y1THdiioZa-h","executionInfo":{"status":"ok","timestamp":1676902294859,"user_tz":180,"elapsed":10,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":94,"outputs":[]},{"cell_type":"code","source":["def predict(net, char, h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","        \n","        # tensor inputs\n","        x = np.array([[net.char2int[char]]])\n","        x = one_hot_encode(x, len(net.chars))\n","        inputs = torch.from_numpy(x)\n","        \n","        if(train_on_gpu):\n","            inputs = inputs.cuda()\n","        \n","        # detach hidden state from history\n","        h = tuple([each.data for each in h])\n","        # get the output of the model\n","        out, h = net(inputs, h)\n","\n","        # get the character probabilities\n","        p = F.softmax(out, dim=1).data\n","        if(train_on_gpu):\n","            p = p.cpu() # move to cpu\n","        \n","        # get top characters\n","        if top_k is None:\n","            top_ch = np.arange(len(net.chars))\n","        else:\n","            p, top_ch = p.topk(top_k)\n","            top_ch = top_ch.numpy().squeeze()\n","        \n","        # select the likely next character with some element of randomness\n","        p = p.numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","        \n","        # return the encoded value of the predicted char and the hidden state\n","        return net.int2char[char], h\n"],"metadata":{"id":"yFzli73oZkKt","executionInfo":{"status":"ok","timestamp":1676902294859,"user_tz":180,"elapsed":9,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["def sample(net, size, prime='The', top_k=None):\n","        \n","    if(train_on_gpu):\n","        net.cuda()\n","    else:\n","        net.cpu()\n","    \n","    net.eval() # eval mode\n","    \n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    h = net.init_hidden(1)\n","    for ch in prime:\n","        char, h = predict(net, ch, h, top_k=top_k)\n","\n","    chars.append(char)\n","    \n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(net, chars[-1], h, top_k=top_k)\n","        chars.append(char)\n","\n","    return ''.join(chars).split('\\n')[0]"],"metadata":{"id":"aciA7_ZqZqc6","executionInfo":{"status":"ok","timestamp":1676902294859,"user_tz":180,"elapsed":9,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["samp = df.sample(10)\n","thai = list(samp.word.values)\n","rom = list(samp.romanization.values)"],"metadata":{"id":"bG8DvaM-bwe4","executionInfo":{"status":"ok","timestamp":1676902294860,"user_tz":180,"elapsed":9,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":97,"outputs":[]},{"cell_type":"code","source":["res = []\n","for th in thai:\n","  res.append(sample(net, 100, prime=th, top_k=5).split()[1])"],"metadata":{"id":"t_gGJ4bnZuAk","executionInfo":{"status":"ok","timestamp":1676902295847,"user_tz":180,"elapsed":996,"user":{"displayName":"S C","userId":"08231442343840734804"}}},"execution_count":98,"outputs":[]},{"cell_type":"code","source":["res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJCgO8zfZ1SX","executionInfo":{"status":"ok","timestamp":1676902295847,"user_tz":180,"elapsed":3,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"54ec3acc-2a72-4386-a600-4d100bf74155"},"execution_count":99,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['binsapkhai',\n"," 'thairuatchamchraochai',\n"," 'khaochaophon',\n"," 'banphangpan',\n"," 'thambotnaekklangkai',\n"," 'phuanokhup-iankhongnang',\n"," 'khomrikthon',\n"," 'kanonkhophaikhongnonyitun',\n"," 'ban-alao',\n"," 'banphaitanphasa']"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["pd.DataFrame([thai,res,rom]).transpose()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"MX96oy-vc0pe","executionInfo":{"status":"ok","timestamp":1676902295847,"user_tz":180,"elapsed":3,"user":{"displayName":"S C","userId":"08231442343840734804"}},"outputId":"abc8d9a2-7f85-416d-ff0e-f9c61348740c"},"execution_count":100,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                       0                          1                       2\n","0                 สับไพ่                 binsapkhai                 sapphai\n","1               ตรวจชำระ      thairuatchamchraochai             truatchamra\n","2                เจ้าพ่อ               khaochaophon                 chaopho\n","3            บ้านพังเป็ด                banphangpan             banphangpet\n","4         ตำบลแก่งกระจาน        thambotnaekklangkai      tambonkaengkrachan\n","5     ผู้ออกเสียงลงคะแนน    phuanokhup-iankhongnang  phu-oksianglongkhanaen\n","6                คริปตอน                khomrikthon                khripton\n","7  นกพงใหญ่พันธุ์ญี่ปุ่น  kanonkhophaikhongnonyitun    nokphongyaiphanyipun\n","8                  อาฬหก                   ban-alao                  alahok\n","9          บ้านไผ่ตาสุ่ม            banphaitanphasa            banphaitasum"],"text/html":["\n","  <div id=\"df-91c0a97c-a58c-48a7-a907-1f40bf39d44d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>สับไพ่</td>\n","      <td>binsapkhai</td>\n","      <td>sapphai</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ตรวจชำระ</td>\n","      <td>thairuatchamchraochai</td>\n","      <td>truatchamra</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>เจ้าพ่อ</td>\n","      <td>khaochaophon</td>\n","      <td>chaopho</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>บ้านพังเป็ด</td>\n","      <td>banphangpan</td>\n","      <td>banphangpet</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ตำบลแก่งกระจาน</td>\n","      <td>thambotnaekklangkai</td>\n","      <td>tambonkaengkrachan</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>ผู้ออกเสียงลงคะแนน</td>\n","      <td>phuanokhup-iankhongnang</td>\n","      <td>phu-oksianglongkhanaen</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>คริปตอน</td>\n","      <td>khomrikthon</td>\n","      <td>khripton</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>นกพงใหญ่พันธุ์ญี่ปุ่น</td>\n","      <td>kanonkhophaikhongnonyitun</td>\n","      <td>nokphongyaiphanyipun</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>อาฬหก</td>\n","      <td>ban-alao</td>\n","      <td>alahok</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>บ้านไผ่ตาสุ่ม</td>\n","      <td>banphaitanphasa</td>\n","      <td>banphaitasum</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91c0a97c-a58c-48a7-a907-1f40bf39d44d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-91c0a97c-a58c-48a7-a907-1f40bf39d44d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-91c0a97c-a58c-48a7-a907-1f40bf39d44d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":[],"metadata":{"id":"kZcPcGvEdEp5"},"execution_count":null,"outputs":[]}]}